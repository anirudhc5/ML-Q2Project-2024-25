# MAXGBoost: A Fast Novel Heuristic Approach to Adaptive Learning Rates in Gradient Boosted Decision Trees

Credit card fraud detection presents unique challenges due to highly complex imbalanced datasets and the need for efficient model training while maintaining high performance. This paper introduces Momentum Approximation XGBoost (MAXGBoost), a novel approach that dynamically adjusts gradient boosted decision tree (GBDT) learning rates depending on the loss landscape. Unlike traditional GBDTs that use static or predetermined decay schedules for the learning rate, MAXGBoost adapts the learning rate based on the loss momentum, reducing the need for hyperparameter tuning while improving convergence speed. We evaluated our approach against Decision Tree, Random Forest, constant learning rate XGBoost, and exponential decay learning rate XGBoost implementations on a real-world credit card transaction dataset containing 284,807 transactions with only 0.2% fraudulent cases. Results demonstrate that MAXGBoost achieves superior precision (0.93827) and accuracy (0.99980) compared to other models, though the exponential decay learning rate approach showed the best recall (0.97531) and AUC (0.98757). Our findings indicate that incorporating momentum approximation  adaptation in GBDTs provides a promising framework for robust fraud detection capabilities.
